{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCI Giuseppe Bonomo BCI2b IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kExkM1UKKKJ-",
    "outputId": "360334b4-6e59-4d0e-b46a-cb0ba6750b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.9.0)\n",
      "Requirement already satisfied: decorator in /Users/giuseppebonomo/Library/Python/3.10/lib/python/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mne) (3.1.2)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mne) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mne) (3.8.4)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mne) (1.26.1)\n",
      "Requirement already satisfied: packaging in /Users/giuseppebonomo/Library/Python/3.10/lib/python/site-packages (from mne) (23.2)\n",
      "Requirement already satisfied: pooch>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: scipy>=1.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mne) (1.13.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mne) (4.66.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.6->mne) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.6->mne) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.6->mne) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.6->mne) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.6->mne) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/giuseppebonomo/Library/Python/3.10/lib/python/site-packages (from matplotlib>=3.6->mne) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/giuseppebonomo/Library/Python/3.10/lib/python/site-packages (from pooch>=1.5->mne) (3.11.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pooch>=1.5->mne) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->mne) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/giuseppebonomo/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: PyWavelets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from PyWavelets) (1.26.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%%\n",
    "#!git clone https://github.com/marco-siino/EEG-ATCNet.git\n",
    "#!git clone https://github.com/Bonomo31/Tesi.git\n",
    "%pip install mne\n",
    "%pip install PyWavelets\n",
    "#Le ulitme versioni non sono compatibili con le librerie usate\n",
    "#in preprocess_HGD.py, quindi si è optato a utilizzare una versione meno recente\n",
    "#%pip install braindecode==0.3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy.signal as signal\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#os.chdir(\"/content/Tesi/BCI2b_IV/\")\n",
    "os.chdir(\"/Users/giuseppebonomo/Desktop/Tesi/BCI2b_IV/\")\n",
    "#os.chdir(\"/Users/Giuseppe/Desktop/Tesi/codice/Tesi/BCI2b_IV/\")\n",
    "\n",
    "\n",
    "import models\n",
    "\n",
    "from preprocess import get_data\n",
    "#from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anMFWoueUv95"
   },
   "source": [
    "# Organize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport requests\\nimport zipfile\\n\\n# Creare la cartella \"dataset\" se non è già presente\\nos.makedirs(\"dataset\", exist_ok=True)\\n\\n# Percorso del file GDF e ZIP\\ngdf_path = \"/Users/Giuseppe/Desktop/Tesi/codice/Tesi/BCI2b_IV/dataset/B0101T.gdf\"\\nzip_path = \"dataset/BCI2b.zip\"\\n\\n# Scaricare il dataset nella cartella \"dataset\" se non è già presente\\nif not os.path.exists(gdf_path):\\n    url = \"https://www.bbci.de/competition/download/competition_iv/BCICIV_2b_gdf.zip\"\\n    print(\"Scaricamento del dataset in corso...\")\\n    \\n    # Scaricare il file ZIP\\n    response = requests.get(url)\\n    with open(zip_path, \"wb\") as f:\\n        f.write(response.content)\\n    \\n    print(\"Estrazione dei file...\")\\n    # Estrarre i file nella cartella \"dataset\"\\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\\n        zip_ref.extractall(\"dataset\")\\n    \\n    # Rimuovere il file .zip\\n    os.remove(zip_path)\\n    print(\"Dataset scaricato ed estratto con successo.\")\\nelse:\\n    print(\"Dataset già scaricato.\")'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Creare la cartella \"dataset\" se non è già presente\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "# Percorso del file GDF e ZIP\n",
    "gdf_path = \"/Users/Giuseppe/Desktop/Tesi/codice/Tesi/BCI2b_IV/dataset/B0101T.gdf\"\n",
    "zip_path = \"dataset/BCI2b.zip\"\n",
    "\n",
    "# Scaricare il dataset nella cartella \"dataset\" se non è già presente\n",
    "if not os.path.exists(gdf_path):\n",
    "    url = \"https://www.bbci.de/competition/download/competition_iv/BCICIV_2b_gdf.zip\"\n",
    "    print(\"Scaricamento del dataset in corso...\")\n",
    "    \n",
    "    # Scaricare il file ZIP\n",
    "    response = requests.get(url)\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    print(\"Estrazione dei file...\")\n",
    "    # Estrarre i file nella cartella \"dataset\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"dataset\")\n",
    "    \n",
    "    # Rimuovere il file .zip\n",
    "    os.remove(zip_path)\n",
    "    print(\"Dataset scaricato ed estratto con successo.\")\n",
    "else:\n",
    "    print(\"Dataset già scaricato.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PM6WBGY4fBe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset già scaricato\n"
     ]
    }
   ],
   "source": [
    "# Creare la cartella \"dataset\" se non è già presente (MacOS)\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "# Scaricare il dataset nella cartella \"dataset\" se non è già presente\n",
    "if not os.path.exists(\"dataset/B0101T.gdf\"):\n",
    "    !wget -O dataset/BCI2b.zip \"https://www.bbci.de/competition/download/competition_iv/BCICIV_2b_gdf.zip\"\n",
    "        \n",
    "    # Estrarre i file nella cartella \"dataset\"\n",
    "    !unzip dataset/BCI2b.zip -d dataset\n",
    "\n",
    "    # Rimuovere il file .zip\n",
    "    os.remove(\"dataset/BCI2b.zip\")\n",
    "else:\n",
    "    print(\"Dataset già scaricato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKIHsRuSU-zn"
   },
   "source": [
    "# Functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n6r2i2nHVDHa"
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "def draw_learning_curves(history, sub):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy - subject: ' + str(sub))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss - subject: ' + str(sub))\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def draw_confusion_matrix(cf_matrix, sub, results_path, classes_labels):\n",
    "    # Generate confusion matrix plot\n",
    "    display_labels = classes_labels\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix,\n",
    "                                display_labels=display_labels)\n",
    "    disp.plot()\n",
    "    disp.ax_.set_xticklabels(display_labels, rotation=12)\n",
    "    plt.title('Confusion Matrix of Subject: ' + sub )\n",
    "    plt.savefig(results_path + '/subject_' + sub + '.png')\n",
    "    plt.show()\n",
    "\n",
    "def draw_performance_barChart(num_sub, metric, label):\n",
    "    fig, ax = plt.subplots()\n",
    "    x = list(range(1, num_sub+1))\n",
    "    ax.bar(x, metric, 0.5, label=label)\n",
    "    ax.set_ylabel(label)\n",
    "    ax.set_xlabel(\"Subject\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_title('Model '+ label + ' per subject')\n",
    "    ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byiRKS6cVQpE"
   },
   "source": [
    "# Preprocessing functions definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBbRpj7xW3P4"
   },
   "source": [
    "## DB4 (Soft -> Threshold 5.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lUw3WgVjXEb6"
   },
   "outputs": [],
   "source": [
    "def db4_soft(signal, wavelet='db4', level=4, threshold=5.5):\n",
    "    \"\"\"\n",
    "    Applica la RDWT (Discrete Wavelet Transform) al segnale e lo ricostruisce,\n",
    "    rimuovendo i coefficienti di dettaglio sotto una certa soglia per enfatizzare\n",
    "    le caratteristiche principali del segnale.\n",
    "\n",
    "    Args:\n",
    "    - signal: il segnale da elaborare\n",
    "    - wavelet: tipo di wavelet da utilizzare (default 'db4')\n",
    "    - level: livello della decomposizione (default 4)\n",
    "    - threshold: soglia per i coefficienti di dettaglio (default 0.5)\n",
    "\n",
    "    Returns:\n",
    "    - Il segnale ricostruito dopo la manipolazione della RDWT\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print(\"Lunghezza originale:\", signal.shape[-1])  # Dovrebbe essere 1125\n",
    "    coeffs = pywt.wavedec(signal, wavelet, mode='per', level=level)\n",
    "    print(\"Lunghezza coefficiente di approssimazione:\", len(coeffs[0]))\n",
    "\n",
    "    reconstructed_signal = pywt.waverec(coeffs, wavelet, mode='per')\n",
    "    print(\"Lunghezza ricostruita:\", len(reconstructed_signal))\n",
    "\n",
    "    # Decomposizione del segnale in coefficienti\n",
    "    coeffs = pywt.wavedec(signal, wavelet, mode='per', level=level)\n",
    "\n",
    "    # Modifica i coefficienti di dettaglio\n",
    "    # Si applica un threshold sui coefficienti di dettaglio per \"ridurre\" la componente di alta frequenza\n",
    "    coeffs_thresholded = [coeffs[0]]  # Mantieni il coefficiente di approssimazione\n",
    "    for i in range(1, len(coeffs)):\n",
    "        coeffs_thresholded.append(np.where(np.abs(coeffs[i]) < threshold, 0, coeffs[i]))\n",
    "\n",
    "    # Ricostruzione del segnale dai coefficienti modificati\n",
    "    reconstructed_signal = pywt.waverec(coeffs_thresholded, wavelet, mode='per')\n",
    "\n",
    "    # Taglia il segnale ricostruito per mantenerne la stessa lunghezza dell'input\n",
    "    return reconstructed_signal[:len(signal)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_7jS5mAW7X8"
   },
   "source": [
    "## DB4 (Hard -> Threshold 10.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lQDSj1UhXQbo"
   },
   "outputs": [],
   "source": [
    "def db4_hard(signal, wavelet='db4', level=4, threshold=10.5):\n",
    "    \"\"\"\n",
    "    Applica la RDWT (Discrete Wavelet Transform) al segnale e lo ricostruisce,\n",
    "    rimuovendo i coefficienti di dettaglio sotto una certa soglia per enfatizzare\n",
    "    le caratteristiche principali del segnale.\n",
    "\n",
    "    Args:\n",
    "    - signal: il segnale da elaborare\n",
    "    - wavelet: tipo di wavelet da utilizzare (default 'db4')\n",
    "    - level: livello della decomposizione (default 4)\n",
    "    - threshold: soglia per i coefficienti di dettaglio (default 0.5)\n",
    "\n",
    "    Returns:\n",
    "    - Il segnale ricostruito dopo la manipolazione della RDWT\n",
    "    \"\"\"\n",
    "\n",
    "    # Decomposizione del segnale in coefficienti\n",
    "    coeffs = pywt.wavedec(signal, wavelet, mode='per', level=level)\n",
    "\n",
    "    # Modifica i coefficienti di dettaglio\n",
    "    # Si applica un threshold sui coefficienti di dettaglio per \"ridurre\" la componente di alta frequenza\n",
    "    coeffs_thresholded = [coeffs[0]]  # Mantieni il coefficiente di approssimazione\n",
    "    for i in range(1, len(coeffs)):\n",
    "        coeffs_thresholded.append(np.where(np.abs(coeffs[i]) < threshold, 0, coeffs[i]))\n",
    "\n",
    "    # Ricostruzione del segnale dai coefficienti modificati\n",
    "    reconstructed_signal = pywt.waverec(coeffs_thresholded, wavelet, mode='per')\n",
    "\n",
    "    # Taglia il segnale ricostruito per mantenerne la stessa lunghezza dell'input\n",
    "    return reconstructed_signal[:len(signal)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NG7RQTKWZjc"
   },
   "source": [
    "## RDWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1l6F2E0iVovM"
   },
   "outputs": [],
   "source": [
    "def rational_dilated_wavelet_transform(sig, wavelet='db4', levels=4, dilation_factors=None, threshold=5.5):\n",
    "    \"\"\"\n",
    "    Applica la Rational Dilated Wavelet Transform (RDWT) a un segnale EEG multidimensionale.\n",
    "\n",
    "    Args:\n",
    "    - sig: array numpy di forma (eventi, 1, canali, campioni)\n",
    "    - wavelet: tipo di wavelet da utilizzare (default 'db4')\n",
    "    - levels: numero di livelli di decomposizione (default 4)\n",
    "    - dilation_factors: lista di fattori di dilatazione razionale (default: [3/2, 5/3, 7/4, ...])\n",
    "    - threshold: soglia per i coefficienti di dettaglio (default 5.5)\n",
    "\n",
    "    Returns:\n",
    "    - Segnale ricostruito con la RDWT (stessa dimensione dell'input)\n",
    "    \"\"\"\n",
    "    eventi, _, canali, campioni = sig.shape\n",
    "    print(\"Lunghezza originale del segnale:\", sig.shape)\n",
    "\n",
    "    if dilation_factors is None:\n",
    "        dilation_factors = [3/2, 5/3, 7/4, 9/5]\n",
    "\n",
    "    coeffs_approx = sig.copy()  # Inizializza con il segnale originale\n",
    "    detail_coeffs = []\n",
    "\n",
    "    for i in range(levels):\n",
    "        factor = dilation_factors[i]\n",
    "        wavelet_filter = pywt.Wavelet(wavelet)\n",
    "        lo_d, hi_d = wavelet_filter.dec_lo, wavelet_filter.dec_hi\n",
    "\n",
    "        # Ridimensionamento dei filtri\n",
    "        lo_d = signal.resample(lo_d, int(len(lo_d) * factor))\n",
    "        hi_d = signal.resample(hi_d, int(len(hi_d) * factor))\n",
    "\n",
    "        print(f\"Livello {i+1}: coeffs_approx shape {coeffs_approx.shape}, lo_d shape {lo_d.shape}, hi_d shape {hi_d.shape}\")\n",
    "\n",
    "        approx = np.zeros_like(coeffs_approx)\n",
    "        detail = np.zeros_like(coeffs_approx)\n",
    "\n",
    "        # Applica la convoluzione per ogni canale EEG\n",
    "        for e in range(eventi):\n",
    "            for c in range(canali):\n",
    "                approx[e, 0, c, :] = np.convolve(coeffs_approx[e, 0, c, :], lo_d, mode='same')\n",
    "                detail[e, 0, c, :] = np.convolve(coeffs_approx[e, 0, c, :], hi_d, mode='same')\n",
    "\n",
    "                # Soglia sui coefficienti di dettaglio\n",
    "                detail[e, 0, c, np.abs(detail[e, 0, c, :]) < threshold] = 0\n",
    "\n",
    "        detail_coeffs.append(detail)\n",
    "        coeffs_approx = approx  # Passa l'approssimazione al livello successivo\n",
    "\n",
    "    # Ricostruzione del segnale\n",
    "    reconstructed = coeffs_approx.copy()\n",
    "    for i in range(levels-1, -1, -1):\n",
    "        reconstructed += detail_coeffs[i]\n",
    "\n",
    "    print(\"Lunghezza del segnale ricostruito:\", reconstructed.shape)\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDWT normalized with min-max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_min_max(sig, reconstructed):\n",
    "    \"\"\"\n",
    "    Normalizza il segnale ricostruito usando la normalizzazione Min-Max,\n",
    "    riportandolo alla scala del segnale di input.\n",
    "\n",
    "    Args:\n",
    "    - sig: array numpy di forma (eventi, 1, canali, campioni), segnale di input\n",
    "    - reconstructed: array numpy di forma (eventi, 1, canali, campioni), segnale ricostruito\n",
    "\n",
    "    Returns:\n",
    "    - Segnale ricostruito normalizzato sulla scala dell'input\n",
    "    \"\"\"\n",
    "    min_input = np.min(sig, axis=-1, keepdims=True)\n",
    "    max_input = np.max(sig, axis=-1, keepdims=True)\n",
    "\n",
    "    min_reconstructed = np.min(reconstructed, axis=-1, keepdims=True)\n",
    "    max_reconstructed = np.max(reconstructed, axis=-1, keepdims=True)\n",
    "\n",
    "    # Evita la divisione per zero\n",
    "    range_reconstructed = max_reconstructed - min_reconstructed\n",
    "    range_reconstructed[range_reconstructed == 0] = 1\n",
    "\n",
    "    normalized = (reconstructed - min_reconstructed) / range_reconstructed\n",
    "\n",
    "    # Riporta alla scala originale\n",
    "    scaled = normalized * (max_input - min_input) + min_input\n",
    "\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8AAXepUzYZOv"
   },
   "outputs": [],
   "source": [
    "def apply_preprocessing (signals_data, preprocessing):\n",
    "  if preprocessing==\"none\":\n",
    "    return signals_data\n",
    "  elif preprocessing==\"db4_soft\":\n",
    "    return db4_soft(signals_data)\n",
    "  elif preprocessing==\"db4_hard\":\n",
    "    return db4_hard(signals_data)\n",
    "  elif preprocessing==\"rdwt\":\n",
    "    return rational_dilated_wavelet_transform(signals_data)\n",
    "  elif preprocessing==\"rdwt_normalized\":\n",
    "    reconstructed = rational_dilated_wavelet_transform(signals_data)\n",
    "    return normalize_min_max(signals_data, reconstructed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcIk3eTxUeqE"
   },
   "source": [
    "# Model training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lUYrFX-SUdQP"
   },
   "outputs": [],
   "source": [
    "#%% Training\n",
    "def train(dataset_conf, train_conf, results_path):\n",
    "\n",
    "    # remove the 'result' folder before training\n",
    "    if os.path.exists(results_path):\n",
    "        # Remove the folder and its contents\n",
    "        shutil.rmtree(results_path)\n",
    "        os.makedirs(results_path)\n",
    "\n",
    "    # Get the current 'IN' time to calculate the overall training time\n",
    "    in_exp = time.time()\n",
    "    # Create a file to store the path of the best model among several runs\n",
    "    best_models = open(results_path + \"/best models RockNetY.txt\", \"w\")\n",
    "    # Create a file to store performance during training\n",
    "    log_write = open(results_path + \"/log RockNetY.txt\", \"w\")\n",
    "\n",
    "    # Get dataset parameters\n",
    "    dataset = dataset_conf.get('name')\n",
    "    n_sub = dataset_conf.get('n_sub')\n",
    "    data_path = dataset_conf.get('data_path')\n",
    "    isStandard = dataset_conf.get('isStandard')\n",
    "    LOSO = dataset_conf.get('LOSO')\n",
    "    # Get training hyperparamters\n",
    "    batch_size = train_conf.get('batch_size')\n",
    "    epochs = train_conf.get('epochs')\n",
    "    signal_preprocessing = dataset_conf.get('signal_preprocessing')\n",
    "    patience = train_conf.get('patience')\n",
    "    lr = train_conf.get('lr')\n",
    "    LearnCurves = train_conf.get('LearnCurves') # Plot Learning Curves?\n",
    "    n_train = train_conf.get('n_train')\n",
    "    model_name = train_conf.get('model')\n",
    "    from_logits = train_conf.get('from_logits')\n",
    "\n",
    "    # Initialize variables\n",
    "    acc = np.zeros((n_sub, n_train))\n",
    "    kappa = np.zeros((n_sub, n_train))\n",
    "\n",
    "    # Iteration over subjects\n",
    "    # for sub in range(n_sub-1, n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
    "    for sub in range(n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
    "\n",
    "        print('\\nTraining on subject ', sub+1)\n",
    "        log_write.write( '\\nTraining on subject '+ str(sub+1) +'\\n')\n",
    "        # Initiating variables to save the best subject accuracy among multiple runs.\n",
    "        BestSubjAcc = 0\n",
    "        bestTrainingHistory = []\n",
    "\n",
    "        # Get training and validation data\n",
    "        X_train, _, y_train_onehot, _, _, _ = get_data(\n",
    "            data_path, sub, dataset, LOSO = LOSO, isStandard = isStandard)\n",
    "\n",
    "        # Divide the training data into training and validation\n",
    "        X_train, X_val, y_train_onehot, y_val_onehot = train_test_split(X_train, y_train_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "        print(\"\\n\\nBefore preprocessing X_train shape is:\"+str(X_train.shape))\n",
    "        print(\"Before preprocessing X_val shape is:\"+str(X_val.shape))\n",
    "        \n",
    "        X_train = apply_preprocessing(X_train, preprocessing=signal_preprocessing)\n",
    "        X_val = apply_preprocessing(X_val, preprocessing=signal_preprocessing)\n",
    "        \n",
    "        print(\"\\n\\nAfter preprocessing X_train shape is:\"+str(X_train.shape))\n",
    "        print(\"After preprocessing X_val shape is:\"+str(X_val.shape))\n",
    "\n",
    "\n",
    "        # Iteration over multiple runs\n",
    "        for train in range(n_train): # How many repetitions of training for subject i.\n",
    "            # Set the random seed for TensorFlow and NumPy random number generator.\n",
    "            # The purpose of setting a seed is to ensure reproducibility in random operations.\n",
    "            tf.random.set_seed(train+1)\n",
    "            np.random.seed(train+1)\n",
    "\n",
    "            # Get the current 'IN' time to calculate the 'run' training time\n",
    "            in_run = time.time()\n",
    "\n",
    "            # Create folders and files to save trained models for all runs\n",
    "            filepath = results_path + '/saved models/run-{}'.format(train+1)\n",
    "            if not os.path.exists(filepath):\n",
    "                os.makedirs(filepath)\n",
    "            filepath = filepath + '/subject-{}.weights.h5'.format(sub+1)\n",
    "\n",
    "            # Create the model\n",
    "            model = getModel(model_name, dataset_conf, from_logits)\n",
    "            # Compile and train the model\n",
    "            model.compile(loss=CategoricalCrossentropy(from_logits=from_logits), optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "\n",
    "            # model.summary()\n",
    "            # plot_model(model, to_file='plot_model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "            callbacks = [\n",
    "                ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\n",
    "                                save_best_only=True, save_weights_only=True, mode='min'),\n",
    "                ReduceLROnPlateau(monitor=\"val_loss\", factor=0.90, patience=20, verbose=0, min_lr=0.0001),\n",
    "                # EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=patience)\n",
    "            ]\n",
    "            history = model.fit(X_train, y_train_onehot, validation_data=(X_val, y_val_onehot),\n",
    "                                epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=0)\n",
    "\n",
    "            # Evaluate the performance of the trained model based on the validation data\n",
    "            # Here we load the Trained weights from the file saved in the hard\n",
    "            # disk, which should be the same as the weights of the current model.\n",
    "            model.load_weights(filepath)\n",
    "            y_pred = model.predict(X_val)\n",
    "\n",
    "            if from_logits:\n",
    "                y_pred = tf.nn.softmax(y_pred).numpy().argmax(axis=-1)\n",
    "            else:\n",
    "                y_pred = y_pred.argmax(axis=-1)\n",
    "\n",
    "            labels = y_val_onehot.argmax(axis=-1)\n",
    "            acc[sub, train]  = accuracy_score(labels, y_pred)\n",
    "            kappa[sub, train] = cohen_kappa_score(labels, y_pred)\n",
    "\n",
    "            # Get the current 'OUT' time to calculate the 'run' training time\n",
    "            out_run = time.time()\n",
    "            # Print & write performance measures for each run\n",
    "            info = 'Subject: {}   seed {}   time: {:.1f} m   '.format(sub+1, train+1, ((out_run-in_run)/60))\n",
    "            info = info + 'valid_acc: {:.4f}   valid_loss: {:.3f}'.format(acc[sub, train], min(history.history['val_loss']))\n",
    "            print(info)\n",
    "            log_write.write(info +'\\n')\n",
    "            # If current training run is better than previous runs, save the history.\n",
    "            if(BestSubjAcc < acc[sub, train]):\n",
    "                 BestSubjAcc = acc[sub, train]\n",
    "                 bestTrainingHistory = history\n",
    "\n",
    "        # Store the path of the best model among several runs\n",
    "        best_run = np.argmax(acc[sub,:])\n",
    "        filepath = '/saved models/run-{}/subject-{}.h5'.format(best_run+1, sub+1)+'\\n'\n",
    "        best_models.write(filepath)\n",
    "\n",
    "        # Plot Learning curves\n",
    "        if (LearnCurves == True):\n",
    "            print('Plot Learning Curves ....... ')\n",
    "            draw_learning_curves(bestTrainingHistory, sub+1)\n",
    "\n",
    "    # Get the current 'OUT' time to calculate the overall training time\n",
    "    out_exp = time.time()\n",
    "\n",
    "    # Print & write the validation performance using all seeds\n",
    "    head1 = head2 = '         '\n",
    "    for sub in range(n_sub):\n",
    "        head1 = head1 + 'sub_{}   '.format(sub+1)\n",
    "        head2 = head2 + '-----   '\n",
    "    head1 = head1 + '  average'\n",
    "    head2 = head2 + '  -------'\n",
    "    info = '\\n---------------------------------\\nValidation performance (acc %):'\n",
    "    info = info + '\\n---------------------------------\\n' + head1 +'\\n'+ head2\n",
    "    for run in range(n_train):\n",
    "        info = info + '\\nSeed {}:  '.format(run+1)\n",
    "        for sub in range(n_sub):\n",
    "            info = info + '{:.2f}   '.format(acc[sub, run]*100)\n",
    "        info = info + '  {:.2f}   '.format(np.average(acc[:, run])*100)\n",
    "    info = info + '\\n---------------------------------\\nAverage acc - all seeds: '\n",
    "    info = info + '{:.2f} %\\n\\nTrain Time  - all seeds: {:.1f}'.format(np.average(acc)*100, (out_exp-in_exp)/(60))\n",
    "    info = info + ' min\\n---------------------------------\\n'\n",
    "    print(info)\n",
    "    log_write.write(info+'\\n')\n",
    "\n",
    "    # Close open files\n",
    "    best_models.close()\n",
    "    log_write.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94bvHJeiVHbe"
   },
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "doqjxvQFVGeB"
   },
   "outputs": [],
   "source": [
    "#%% Evaluation\n",
    "def test(model, dataset_conf, results_path, allRuns = True):\n",
    "    # Open the  \"Log\" file to write the evaluation results\n",
    "    log_write = open(results_path + \"/log RockNetY.txt\", \"a\")\n",
    "\n",
    "    # Get dataset paramters\n",
    "    dataset = dataset_conf.get('name')\n",
    "    signal_preprocessing = dataset_conf.get('signal_preprocessing')\n",
    "    n_classes = dataset_conf.get('n_classes')\n",
    "    n_sub = dataset_conf.get('n_sub')\n",
    "    data_path = dataset_conf.get('data_path')\n",
    "    isStandard = dataset_conf.get('isStandard')\n",
    "    LOSO = dataset_conf.get('LOSO')\n",
    "    classes_labels = dataset_conf.get('cl_labels')\n",
    "    #Ripetuto\n",
    "    #signal_preprocessing = dataset_conf.get('signal_preprocessing')\n",
    "\n",
    "    # Test the performance based on several runs (seeds)\n",
    "    runs = os.listdir(results_path+\"/saved models\")\n",
    "    # Initialize variables\n",
    "    acc = np.zeros((n_sub, len(runs)))\n",
    "    kappa = np.zeros((n_sub, len(runs)))\n",
    "    cf_matrix = np.zeros([n_sub, len(runs), n_classes, n_classes])\n",
    "\n",
    "    # Iteration over subjects\n",
    "    # for sub in range(n_sub-1, n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
    "    inference_time = 0 #  inference_time: classification time for one trial\n",
    "    for sub in range(n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
    "        # Load data\n",
    "        _, _, _, X_test, _, y_test_onehot = get_data(data_path, sub, dataset, LOSO = LOSO, isStandard = isStandard)\n",
    "\n",
    "        X_test = apply_preprocessing(X_test, preprocessing=signal_preprocessing)\n",
    "\n",
    "        # Iteration over runs (seeds)\n",
    "        for seed in range(len(runs)):\n",
    "            # Load the model of the seed.\n",
    "            model.load_weights('{}/saved models/{}/subject-{}.weights.h5'.format(results_path, runs[seed], sub+1))\n",
    "\n",
    "            inference_time = time.time()\n",
    "            # Predict MI task\n",
    "            y_pred = model.predict(X_test).argmax(axis=-1)\n",
    "            inference_time = (time.time() - inference_time)/X_test.shape[0]\n",
    "            # Calculate accuracy and K-score\n",
    "            labels = y_test_onehot.argmax(axis=-1)\n",
    "            acc[sub, seed]  = accuracy_score(labels, y_pred)\n",
    "            kappa[sub, seed] = cohen_kappa_score(labels, y_pred)\n",
    "            # Calculate and draw confusion matrix\n",
    "            cf_matrix[sub, seed, :, :] = confusion_matrix(labels, y_pred, normalize='true')\n",
    "            # draw_confusion_matrix(cf_matrix[sub, seed, :, :], str(sub+1), results_path, classes_labels)\n",
    "\n",
    "    # Print & write the average performance measures for all subjects\n",
    "    head1 = head2 = '                  '\n",
    "    for sub in range(n_sub):\n",
    "        head1 = head1 + 'sub_{}   '.format(sub+1)\n",
    "        head2 = head2 + '-----   '\n",
    "    head1 = head1 + '  average'\n",
    "    head2 = head2 + '  -------'\n",
    "    info = '\\n' + head1 +'\\n'+ head2\n",
    "    info = '\\n---------------------------------\\nTest performance (acc & k-score):\\n'\n",
    "    info = info + '---------------------------------\\n' + head1 +'\\n'+ head2\n",
    "    for run in range(len(runs)):\n",
    "        info = info + '\\nSeed {}: '.format(run+1)\n",
    "        info_acc = '(acc %)   '\n",
    "        info_k = '        (k-sco)   '\n",
    "        for sub in range(n_sub):\n",
    "            info_acc = info_acc + '{:.2f}   '.format(acc[sub, run]*100)\n",
    "            info_k = info_k + '{:.3f}   '.format(kappa[sub, run])\n",
    "        info_acc = info_acc + '  {:.2f}   '.format(np.average(acc[:, run])*100)\n",
    "        info_k = info_k + '  {:.3f}   '.format(np.average(kappa[:, run]))\n",
    "        info = info + info_acc + '\\n' + info_k\n",
    "    info = info + '\\n----------------------------------\\nAverage - all seeds (acc %): '\n",
    "    info = info + '{:.2f}\\n                    (k-sco): '.format(np.average(acc)*100)\n",
    "    info = info + '{:.3f}\\n\\nInference time: {:.2f}'.format(np.average(kappa), inference_time * 1000)\n",
    "    info = info + ' ms per trial\\n----------------------------------\\n'\n",
    "    print(info)\n",
    "    log_write.write(info+'\\n')\n",
    "\n",
    "    # Draw a performance bar chart for all subjects\n",
    "    draw_performance_barChart(n_sub, acc.mean(1), 'Accuracy')\n",
    "    draw_performance_barChart(n_sub, kappa.mean(1), 'k-score')\n",
    "    # Draw confusion matrix for all subjects (average)\n",
    "    draw_confusion_matrix(cf_matrix.mean((0,1)), 'All', results_path, classes_labels)\n",
    "    # Close opened file\n",
    "    log_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weCl_i-ZVdlL"
   },
   "source": [
    "# Model selection and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5gAM65lQSTH1"
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "def getModel(model_name, dataset_conf, from_logits = False):\n",
    "\n",
    "    n_classes = dataset_conf.get('n_classes')\n",
    "    n_channels = dataset_conf.get('n_channels')\n",
    "    in_samples = dataset_conf.get('in_samples')\n",
    "\n",
    "    # Select the model\n",
    "    if(model_name == 'RockNet'):\n",
    "        # Train using the proposed ATCNet model: https://ieeexplore.ieee.org/document/9852687\n",
    "        model = models.RockNet_(\n",
    "            # Dataset parameters\n",
    "            n_classes = n_classes,\n",
    "            in_chans = n_channels,\n",
    "            in_samples = in_samples,\n",
    "            # Sliding window (SW) parameter\n",
    "            n_windows = 5,\n",
    "            # Attention (AT) block parameter\n",
    "            attention = 'mha', # Options: None, 'mha','mhla', 'cbam', 'se'\n",
    "            # Convolutional (CV) block parameters\n",
    "            eegn_F1 = 16,\n",
    "            eegn_D = 2,\n",
    "            eegn_kernelSize = 64,\n",
    "            eegn_poolSize = 7,\n",
    "            eegn_dropout = 0.5,\n",
    "            # Temporal convolutional (TC) block parameters\n",
    "            tcn_depth = 2,\n",
    "            tcn_kernelSize = 4,\n",
    "            tcn_filters = 64,\n",
    "            tcn_dropout = 0.3,\n",
    "            tcn_activation='elu',\n",
    "            )\n",
    "    elif(model_name == 'RockNetA'):\n",
    "        model = models.RockNetA(\n",
    "            n_classes = n_classes,\n",
    "            in_chans = n_channels,\n",
    "            in_samples = in_samples,\n",
    "            n_windows = 5,\n",
    "            attention = 'mha',\n",
    "            eegn_F1 = 16,\n",
    "            eegn_D = 2,\n",
    "            eegn_kernelSize = 64,\n",
    "            eegn_poolSize = 7,\n",
    "            eegn_dropout = 0.5,\n",
    "            tcn_depth = 2,\n",
    "            tcn_kernelSize = 4,\n",
    "            tcn_filters = 32,\n",
    "            tcn_dropout = 0.3,\n",
    "            tcn_activation='elu',\n",
    "        )\n",
    "    elif(model_name == 'RockNetB'):\n",
    "        model = models.RockNetB(\n",
    "            n_classes = n_classes,\n",
    "            in_chans = n_channels,\n",
    "            in_samples = in_samples,\n",
    "            n_windows = 5,\n",
    "            attention = 'mha',\n",
    "            eegn_F1 = 16,\n",
    "            eegn_D = 2,\n",
    "            eegn_kernelSize = 64,\n",
    "            eegn_poolSize = 7,\n",
    "            eegn_dropout = 0.5,\n",
    "            tcn_depth = 2,\n",
    "            tcn_kernelSize = 4,\n",
    "            tcn_filters = 32,\n",
    "            tcn_dropout = 0.3,\n",
    "            tcn_activation='elu',\n",
    "        )\n",
    "    elif(model_name == 'RockNetC'):\n",
    "        model = models.RockNetC(\n",
    "            n_classes = n_classes,\n",
    "            in_chans = n_channels,\n",
    "            in_samples = in_samples,\n",
    "            n_windows = 5,\n",
    "            attention = 'mha',\n",
    "            eegn_F1 = 16,\n",
    "            eegn_D = 2,\n",
    "            eegn_kernelSize = 64,\n",
    "            eegn_poolSize = 7,\n",
    "            eegn_dropout = 0.5,\n",
    "            tcn_depth = 2,\n",
    "            tcn_kernelSize = 4,\n",
    "            tcn_filters = 32,\n",
    "            tcn_dropout = 0.3,\n",
    "            tcn_activation='elu',\n",
    "        )\n",
    "    elif(model_name == 'RockNetX'):\n",
    "        model = models.RockNetX(\n",
    "            n_classes = n_classes,\n",
    "            in_chans = n_channels,\n",
    "            in_samples = in_samples,\n",
    "            n_windows = 5,\n",
    "            attention = 'transformer',\n",
    "            eegn_F1 = 16,\n",
    "            eegn_D = 2,\n",
    "            eegn_kernelSize = 64,\n",
    "            eegn_poolSize = 7,\n",
    "            eegn_dropout = 0.5,\n",
    "            tcn_depth = 3,\n",
    "            tcn_kernelSize = 4,\n",
    "            tcn_filters = 64,\n",
    "            tcn_dropout = 0.3,\n",
    "            tcn_activation='gelu',\n",
    "        )\n",
    "    elif(model_name == 'RockNetY'):\n",
    "        model = models.RockNetY(\n",
    "            n_classes = n_classes,\n",
    "            in_chans = n_channels,\n",
    "            in_samples = in_samples,\n",
    "            n_windows = 5,\n",
    "            attention = 'mha',\n",
    "            eegn_F1 = 16,\n",
    "            eegn_D = 2,\n",
    "            eegn_kernelSize = 64,\n",
    "            eegn_poolSize = 7,\n",
    "            eegn_dropout = 0.5,\n",
    "            tcn_depth = 2,\n",
    "            tcn_kernelSize = 4,\n",
    "            tcn_filters = 32,\n",
    "            tcn_dropout = 0.3,\n",
    "            tcn_activation='elu',\n",
    "        )\n",
    "    elif(model_name == 'RockNetZ'):\n",
    "        model = models.RockNetZ(\n",
    "            n_classes = n_classes,\n",
    "            in_chans = n_channels,\n",
    "            in_samples = in_samples,\n",
    "            n_windows = 5,\n",
    "            attention = 'mha',\n",
    "            eegn_F1 = 16,\n",
    "            eegn_D = 2,\n",
    "            eegn_kernelSize = 64,\n",
    "            eegn_poolSize = 7,\n",
    "            eegn_dropout = 0.5,\n",
    "            tcn_depth = 2,\n",
    "            tcn_kernelSize = 4,\n",
    "            tcn_filters = 32,\n",
    "            tcn_dropout = 0.3,\n",
    "            tcn_activation='relu',\n",
    "        )\n",
    "    elif(model_name == 'ATCNet'):\n",
    "        # Train using the proposed ATCNet model: https://ieeexplore.ieee.org/document/9852687\n",
    "        model = models.ATCNet_(\n",
    "            # Dataset parameters\n",
    "            n_classes = n_classes,\n",
    "            in_chans = n_channels,\n",
    "            in_samples = in_samples,\n",
    "            # Sliding window (SW) parameter\n",
    "            n_windows = 5,\n",
    "            # Attention (AT) block parameter\n",
    "            attention = 'mha', # Options: None, 'mha','mhla', 'cbam', 'se'\n",
    "            # Convolutional (CV) block parameters\n",
    "            eegn_F1 = 16,\n",
    "            eegn_D = 2,\n",
    "            eegn_kernelSize = 64,\n",
    "            eegn_poolSize = 7,\n",
    "            eegn_dropout = 0.3,\n",
    "            # Temporal convolutional (TC) block parameters\n",
    "            tcn_depth = 2,\n",
    "            tcn_kernelSize = 4,\n",
    "            tcn_filters = 32,\n",
    "            tcn_dropout = 0.3,\n",
    "            tcn_activation='elu',\n",
    "            )\n",
    "    elif(model_name == 'TCNet_Fusion'):\n",
    "        # Train using TCNet_Fusion: https://doi.org/10.1016/j.bspc.2021.102826\n",
    "        model = models.TCNet_Fusion(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n",
    "    elif(model_name == 'EEGTCNet'):\n",
    "        # Train using EEGTCNet: https://arxiv.org/abs/2006.00622\n",
    "        model = models.EEGTCNet(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n",
    "    elif(model_name == 'EEGNet'):\n",
    "        # Train using EEGNet: https://arxiv.org/abs/1611.08024\n",
    "        model = models.EEGNet_classifier(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n",
    "    elif(model_name == 'EEGNeX'):\n",
    "        # Train using EEGNeX: https://arxiv.org/abs/2207.12369\n",
    "        model = models.EEGNeX_8_32(n_timesteps = in_samples , n_features = n_channels, n_outputs = n_classes)\n",
    "    elif(model_name == 'DeepConvNet'):\n",
    "        # Train using DeepConvNet: https://doi.org/10.1002/hbm.23730\n",
    "        model = models.DeepConvNet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
    "    elif(model_name == 'ShallowConvNet'):\n",
    "        # Train using ShallowConvNet: https://doi.org/10.1002/hbm.23730\n",
    "        model = models.ShallowConvNet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
    "    elif(model_name == 'MBEEG_SENet'):\n",
    "        # Train using MBEEG_SENet: https://www.mdpi.com/2075-4418/12/4/995\n",
    "        model = models.MBEEG_SENet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"'{}' model is not supported yet!\".format(model_name))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PE_qndEAVZ-_"
   },
   "source": [
    "# Run the simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkiYgl7FDPqN"
   },
   "source": [
    "## Define the run function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nWF0BbQzVbdN"
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "def run(signal_preprocessing):\n",
    "    # Define dataset parameters\n",
    "    #dataset = 'BCI2a' # Options: 'BCI2a','BCI2b','HGD'\n",
    "    dataset = 'BCI2b'\n",
    "\n",
    "    if dataset == 'BCI2a':\n",
    "        in_samples = 1125\n",
    "        n_channels = 22\n",
    "        n_sub = 9\n",
    "        n_classes = 4\n",
    "        classes_labels = ['Left hand', 'Right hand','Foot','Tongue']\n",
    "        data_path = 'dataset/' #os.path.expanduser('~') + '/BCI Competition IV/BCI Competition IV-2a/BCI Competition IV 2a mat/'\n",
    "    elif dataset == 'HGD':\n",
    "        in_samples = 1125\n",
    "        n_channels = 44\n",
    "        n_sub = 14\n",
    "        n_classes = 4\n",
    "        classes_labels = ['Right Hand', 'Left Hand','Rest','Feet']\n",
    "        data_path = 'dataset/' #os.path.expanduser('~') + '/mne_data/MNE-schirrmeister2017-data/robintibor/high-gamma-dataset/raw/master/data/'\n",
    "    elif dataset == 'BCI2b':\n",
    "        in_samples = 750\n",
    "        n_channels = 3\n",
    "        n_sub = 9\n",
    "        n_classes = 2\n",
    "        classes_labels = ['Left hand', 'Right hand']\n",
    "        data_path = 'dataset/' #os.path.expanduser('~') + '/BCI Competition IV/BCI Competition IV-2b/BCI Competition IV 2b.mat'\n",
    "    else:\n",
    "        raise Exception(\"'{}' dataset is not supported yet!\".format(dataset))\n",
    "\n",
    "    # Create a folder to store the results of the experiment\n",
    "    results_path = os.getcwd() + \"/results_RockNetY\"\n",
    "    if not  os.path.exists(results_path):\n",
    "      os.makedirs(results_path)   # Create a new directory if it does not exist\n",
    "\n",
    "    # Set dataset paramters\n",
    "    dataset_conf = { 'name': dataset, 'n_classes': n_classes, 'cl_labels': classes_labels,\n",
    "                    'n_sub': n_sub, 'n_channels': n_channels, 'in_samples': in_samples,\n",
    "                    'data_path': data_path, 'isStandard': True, 'LOSO': False,\n",
    "                     'signal_preprocessing':signal_preprocessing}\n",
    "\n",
    "    # Set training hyperparamters\n",
    "    #AUMENTARE LE EPOCHE A 1000\n",
    "    train_conf = { 'batch_size': 64, 'epochs': 1000, 'patience': 100, 'lr': 0.001,'n_train': 1,\n",
    "                  'LearnCurves': True, 'from_logits': False, 'model':'RockNetY'}\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    train(dataset_conf, train_conf, results_path)\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    # Evaluate the model based on the weights saved in the '/results' folder\n",
    "    model = getModel(train_conf.get('model'), dataset_conf)\n",
    "    test(model, dataset_conf, results_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKAI_oyNBGVd"
   },
   "source": [
    "## No preprocessing on EEG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "-8J-ZBUXCr-3",
    "outputId": "1ea1b7f4-601f-44b0-b349-d499ea12143d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "\n",
      "Training on subject  1\n",
      "Extracting EDF parameters from /Users/giuseppebonomo/Desktop/Tesi/BCI2b_IV/dataset/B0101T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4, EOG:ch01, EOG:ch02, EOG:ch03\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuseppebonomo/Desktop/Tesi/BCI2b_IV/preprocess.py:87: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw = mne.io.read_raw_gdf(gdf_file, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 1e+02 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 100.00 Hz\n",
      "- Upper transition bandwidth: 25.00 Hz (-6 dB cutoff frequency: 112.50 Hz)\n",
      "- Filter length: 1651 samples (6.604 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Extracting EDF parameters from /Users/giuseppebonomo/Desktop/Tesi/BCI2b_IV/dataset/B0102T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4, EOG:ch01, EOG:ch02, EOG:ch03\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 552451  =      0.000 ...  2209.804 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuseppebonomo/Desktop/Tesi/BCI2b_IV/preprocess.py:87: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw = mne.io.read_raw_gdf(gdf_file, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 1e+02 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 100.00 Hz\n",
      "- Upper transition bandwidth: 25.00 Hz (-6 dB cutoff frequency: 112.50 Hz)\n",
      "- Filter length: 1651 samples (6.604 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Used Annotations descriptions: ['1023', '32766', '768', '769', '770']\n",
      "Extracting EDF parameters from /Users/giuseppebonomo/Desktop/Tesi/BCI2b_IV/dataset/B0103T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4, EOG:ch01, EOG:ch02, EOG:ch03\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 469010  =      0.000 ...  1876.040 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuseppebonomo/Desktop/Tesi/BCI2b_IV/preprocess.py:87: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw = mne.io.read_raw_gdf(gdf_file, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 1e+02 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 100.00 Hz\n",
      "- Upper transition bandwidth: 25.00 Hz (-6 dB cutoff frequency: 112.50 Hz)\n",
      "- Filter length: 1651 samples (6.604 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Extracting EDF parameters from /Users/giuseppebonomo/Desktop/Tesi/BCI2b_IV/dataset/B0104E.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4, EOG:ch01, EOG:ch02, EOG:ch03\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 466313  =      0.000 ...  1865.252 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuseppebonomo/Desktop/Tesi/BCI2b_IV/preprocess.py:87: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw = mne.io.read_raw_gdf(gdf_file, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 1e+02 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 100.00 Hz\n",
      "- Upper transition bandwidth: 25.00 Hz (-6 dB cutoff frequency: 112.50 Hz)\n",
      "- Filter length: 1651 samples (6.604 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '781', '783']\n",
      "Extracting EDF parameters from /Users/giuseppebonomo/Desktop/Tesi/BCI2b_IV/dataset/B0105E.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4, EOG:ch01, EOG:ch02, EOG:ch03\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 474362  =      0.000 ...  1897.448 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuseppebonomo/Desktop/Tesi/BCI2b_IV/preprocess.py:87: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw = mne.io.read_raw_gdf(gdf_file, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 1e+02 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 100.00 Hz\n",
      "- Upper transition bandwidth: 25.00 Hz (-6 dB cutoff frequency: 112.50 Hz)\n",
      "- Filter length: 1651 samples (6.604 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '781', '783']\n",
      "y_train_onehot shape: (84, 2)\n",
      "y_test_onehot shape: (94, 2)\n",
      "\n",
      "\n",
      "Before preprocessing X_train shape is:(67, 1, 3, 750)\n",
      "Before preprocessing X_val shape is:(17, 1, 3, 750)\n",
      "\n",
      "\n",
      "After preprocessing X_train shape is:(67, 1, 3, 750)\n",
      "After preprocessing X_val shape is:(17, 1, 3, 750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 49\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(signal_preprocessing)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining the model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Evaluate the model based on the weights saved in the '/results' folder\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 96\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset_conf, train_conf, results_path)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# model.summary()\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# plot_model(model, to_file='plot_model.png', show_shapes=True, show_layer_names=True)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     91\u001b[0m     ModelCheckpoint(filepath, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     92\u001b[0m                     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     93\u001b[0m     ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.90\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m),\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=patience)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m ]\n\u001b[0;32m---> 96\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_onehot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of the trained model based on the validation data\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Here we load the Trained weights from the file saved in the hard\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# disk, which should be the same as the weights of the current model.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(filepath)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run(\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmgl52x4BRnm"
   },
   "source": [
    "##  DB4 (Soft) Preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKWBjWr9BWb5"
   },
   "outputs": [],
   "source": [
    "run(\"db4_soft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcAuThEQBWy3"
   },
   "source": [
    "## DB4 (Hard) Preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6vR-NHTBaLg"
   },
   "outputs": [],
   "source": [
    "run(\"db4_hard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sas6wn-n9bVV"
   },
   "source": [
    "## RDWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AanpQPu49dhp"
   },
   "outputs": [],
   "source": [
    "run(\"rdwt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDWT Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"rdwt_normalized\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
